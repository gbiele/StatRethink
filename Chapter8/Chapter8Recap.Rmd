---
title: "Chapter 8: Recap"
author: "Guido Biele"
date: "11.04.2024"
output:
  html_document: 
    mathjax: default
    toc: true
    toc_depth: 2
    code_folding: hide
header-includes: 
    \usepackage{xcolor}
    \usepackage{amsmath}
---


```{css, echo = F}
body{
  font-family: Helvetica;
  font-size: 16pt;
  max-width: 1000px;
  margin: auto;
  margin-left:310px;
}
pre{
  font-size: 20px;
}
/* Headers */
h1{
    font-size: 24pt;
  }
h1,h2{
    font-size: 20pt;
  }
h3,h4,h5,h6{
  font-size: 18pt;
}

#TOC {
  position: fixed;
  left: 0;
  top: 0;
  width: 300px;
  height: 100%;
  overflow:auto;
}

img {
    display: block;
    float: none;
    margin-left: auto;
    margin-right: auto;
}
```

```{css sidenote, echo = FALSE}
.sidenote, .marginnote { 
  float: right;
  clear: right;
  margin-right: -50%;
  width: 40%;         # best between 50% and 60%
  margin-top: 0;
  margin-bottom: 0;
  line-height: 2;
  font-size: 1.8rem;
  vertical-align: baseline;
  position: relative;
  }
```

  
```{css, echo = F}
body{
  font-family: Helvetica;
  font-size: 16pt;
}
pre{
  font-size: 20px;
}
/* Headers */
h1{
    font-size: 24pt;
  }
h1,h2{
    font-size: 20pt;
  }
h3,h4,h5,h6{
  font-size: 18pt;
}
```

```{r setup, include=FALSE, message=FALSE, warning=FALSE, results='hide'}
knitr::opts_chunk$set(echo = TRUE, dpi = 300, global.par = TRUE)

library(rethinking)
library(magrittr)
library(knitr)
library(kableExtra)
library(MASS)
source("../utils.R")
```

# The Scarr-Rowe effect

We are going to use the [Scarr-Rowe effect](https://en.wikipedia.org/wiki/Scarr%E2%80%93Rowe_effect) (or hypothesis) to look more closely on interaction effects. The Scarr-Rowe hypothesis states that the (genetic) **heritability of a trait depends on the environment**, such that the effects of genes are larger when environments are better. The underlying idea is that if everyone lives in a perfect environment, i.e. there is no variation in the relevant environment, then a trait will only depend on genes.

This interaction can be visualized as follows:

```{r fig.height=4, fig.width=8, fig.align = 'center'}
par(mfrow = c(1,2), mar=c(3,3,1,.5), mgp=c(1.25,.5,0), tck=-.01)

plot(0,type = "n", ylim = c(0,1), xlim = c(0,1), xaxt = "n", yaxt = "n",
     ylab = "IQ", xlab = "PGS", bty = "n")
axis(1, at = c(-2,2))
axis(2, at = c(-2,2))
lines(c(.05,.95), c(.3, .8), col = "green3", lwd=2)
lines(c(.05,.95), c(.2, .5), col = "orange3", lwd=2)
legend("topleft", title = "environment", bty = "n",
       legend = c("scarce","plentiful"),
       lwd=2, col = c("orange3","green3"))

heritability = c(scarce = 0.3, plentiful = 0.5)
barplot(heritability,
        ylim = c(0,1), yaxt = "n",
        xlab = "environment",
        ylab = "heritability (effect of genes)",
        col = c("orange3","green3"))
axis(2, at = c(-2,2))
axis(1, at = c(-2,3))
```


Here is a DAG that describes such a model, where

- $\small A$ = additive genetic effects on child IQ
- $\small SES$ = parental socioeconomic status
- $\small IQ$ = child's intelligence quotient

and the Scarr-Rowe effect means the the coefficient of the path $\small A \rightarrow IQ$ depends on $\small SES$.

```{r fig.height=3, fig.width=3, fig.align = 'center', out.width="40%"}
library(dagitty) 
dag = dagitty(
  "dag{
  IQ;
  A -> IQ;
  SES -> IQ;
  }")
coord.list = 
  list(
    x=c(A=0,SES=0,IQ=1),
    y=c(A=-.5,SES=.5,IQ=0))
coordinates(dag) = coord.list
drawdag(dag, cex = 1.5)
```


**Note that DAGs do not encode interaction effects by drawing an arrow from the moderator to the relevant path.** Instead, there is a path from the moderator to the outcome variable. So the DAG only tells us that IQ is a function of two other variables $\small IQ = f(A,SES)$, but it does not tell us what the function $f()$ is. 

This function could be 

- $\small IQ = A + SES$, 
- $\small IQ = A \cdot SES$ or 
- $\small IQ = SES + A \cdot SES$,
- ...

or any other imaginable function that uses the variables. The model we have now in mind is:

$$
IQ =  f(SES) A
$$

## Scarr-Rowe simulation

Lets simulate data that show the Scarr-Rowe effect, first our exogenous variables. To keep things simple, we assume that the variables are independent and close to normally distributed:

```{r class.source = 'fold-show'}
set.seed(25)
N = 250
SES = rlnorm(N,log(4.75),.3)
A = rlnorm(N,log(10),.2)
```

```{r echo = FALSE, fig.width=3, fig.height=3, out.width="40%", fig.cap="A and SES are independent"}
par(mfrow = c(1,1),mar=c(3,3,1,.5), mgp=c(1.75,.5,0), tck=-.01)
plot(SES,A, pch = 16, cex = .5)
abline(lm(A~SES), col = "grey")
```


Now comes the interesting part: Scarr-Rowe assumes that the effect or weight of genes depends on the SES. So we formulate the weight of genes as as a function of SES. 

```{r class.source = 'fold-show', warning=FALSE, message=FALSE}
library(boot)
# use inv.logit to give weights a lower and upper bound
# add a constant one to model that even at lowest SES 
# there are genetic effects
b_A = function(x) 0.25 + inv.logit(x-5) * 2.75
```


We are literally defining the weight for A as a function of SES. This is one way to understand interactions. Here is a visualization of the weights, with a histogram of SES values on the bottom.

```{r fig.height=3.5, fig.width=5, fig.align = 'center', out.width="70%"}
par(mar=c(3,3,.5,.5), mgp=c(1.75,.5,0), tck=-.01)
curve(b_A(x),0,10, 
      ylab = expression("effect size"~beta[A]), 
      ylim = c(0,b_A(10)), xlab = "SES")
hist(SES,add = T, probability = T)
```
**In this plot, the interaction is encoded by the fact that $\small \beta_A$ has a slope.**

Now we can simulate IQ values using 

1. exogenous variables and 
2. derived weights. 

We assume that IQ depends on $\small A$, whereby this effect depends on $\small SES$: $\small IQ = f(SES)A$. Lets simulate data from this model:

```{r class.source = 'fold-show'}
# Adding 85 to get an IQ around 100
IQ = 85 + b_A(SES)*A + rnorm(N,sd = 10)
```

```{r echo = FALSE, fig.height=4, fig.width=4, fig.align = 'center', out.width="50%"}
par(mar=c(3,3,.5,.5), mgp=c(1.75,.5,0), tck=-.01)
hist(IQ, main = "")
```


If we just look at the bivariate associations between  $\small A$ or $\small SES$ and $\small IQ$, we get the following plots:

```{r echo = FALSE, fig.height=4, fig.width=8, fig.align = 'center', out.width="100%"}
par(mfrow = c(1,2),mar=c(3,3,1,.5), mgp=c(1.75,.5,0), tck=-.01)
plot(SES,IQ, pch = 16, cex = .5)
abline(lm(IQ~SES), col = "grey")
plot(A,IQ, pch = 16, cex = .5)
abline(lm(IQ~A), col = "grey")
```

# Categorial interactions

To run a simple interaction with a categorical interaction variable we dichotomise the SES variable to create the variable `SES.c`:

```{r class.source = 'fold-show'}
dt = data.frame(
  A = A, IQ = IQ, SES = SES,
  SES.c = 1 + (SES > quantile(SES,.50)) )
```


Lets plot the association between  $\small A$ and $\small IQ$ again, this time split by SES:

```{r fig.height=3.5, fig.width=3.5, fig.align = 'center', out.width="50%"}
low.SES = dt$SES.c == 1
high.SES = dt$SES.c == 2
plot_data = function(dt) {
  par(mar=c(3,3,1,.5), mgp=c(1.75,.5,0), tck=-.01)
  plot(IQ ~ A, data = dt, col = dt$SES.c, pch = 16, cex = .5,
       ylim = range(dt$IQ), ylab = "IQ", xlab = "A")
  legend("topleft", pch = 16, col = c("black","red"),
         legend = c("low SES","high SES"), bty = "n")
  
}
plot_data(dt)
#abline(lm(IQ ~ A, data = dt[low.SES,]), col = "black")
#abline(lm(IQ ~ A, data = dt[high.SES,]), col = "red")
```

Next, we estimate some linear regression models with increasing complexity. Our goal is to have a model that accurately depicts the effect of A and SES on the IQ.

## Simple linear regression

We start with a simple linear regression. But first some intuitions on priors:

- Eyeballing the data, we see that the IQ at an average $\small A$ of 10 is around 100, so we use a prior `a ~ dnorm(100,5)`
- The range of $\small A$ is 16-6 = 10 and the range of IQ = 130-70 = 60. So for a one unit increase of $\small A$, IQ can change by around 60/10 = 6. If we want that an effect of +/-6 is well within 1 sd of the prior, we set the prior for the effect of $\small A$ to `a ~ dnorm(0,6)`. This prior allows for the possibility of a negative effect of $\small A$. 
- Lacking a strong intuition for the error variance, we set the prior for the variance to a generous `dexp(0.5)` 

Here is the regression model:

```{r class.source = 'fold-show'}
set.seed(1)
IQ.A = 
  quap(
    alist(
      IQ ~ dnorm(mu,sigma),
      mu <- a + b*(A - 10),
      a ~ dnorm(100,5),
      b ~ dnorm(0,6),
      sigma ~ dexp(.5)
    ),
    data=dt)
# IQ ~ A
```

Lets quickly look at the prior predictions to make sure the piors are OK.

```{r 'fold-show', fig.height=4, fig.width=4, fig.align = 'center', out.width="50%"}
prior = extract.prior(IQ.A)
A.seq = seq(from=0,to=20,length.out=50)
mu = link(
  IQ.A,
  post=prior,
  data=data.frame(A=A.seq))

par(mar=c(3,3,1,.5), mgp=c(1.75,.5,0), tck=-.01)
plot(
  0,type = "n", ylab = "IQ", xlab = "A", 
  xlim = c(min(A),max(A)),
  ylim = c(70,130))
matlines(
  A.seq,t(mu[1:100,]),type = "l", lty = 1,
  col = adjustcolor("blue", alpha = .5))
```

Yes, this looks good.

Here are the posterior predictions:

```{r fig.height=4, fig.width=4, fig.align = 'center', out.width="50%"}
plot_data(dt)
plot_mu.CIs(IQ.A, data.frame(A=A.seq), "blue", spaghetti = TRUE)
```

This looks generally fine, but we are not capturing the group effects.

## Linear regression with category-main effect

To fit a model with a main effect of education, we use the indexing approach:

```{r class.source = 'fold-show'}
set.seed(1)
IQ.A_SES = 
  quap(
    alist(
      IQ ~ dnorm(mu,sigma),
      mu <- a[SES.c] + b*(A - 10),
      a[SES.c] ~ dnorm(100,5),
      b ~ dnorm(0,4),
      sigma ~ dexp(.5)
    ),
    data=dt)
# IQ ~ 0 + SES.c + A
```


```{r fig.height=4, fig.width=4, fig.align = 'center', out.width="50%"}
plot_data(dt)
plot_mu.CIs(IQ.A_SES, data.frame(A=A.seq, SES.c = 1), "black", spaghetti = TRUE)
plot_mu.CIs(IQ.A_SES, data.frame(A=A.seq, SES.c = 2), "red", spaghetti = TRUE)
```

We can see already from the plot that the model with separate means for high and low SES is better. But here is the comparison with PSIS-LOO:

```{r class.source = 'fold-show'}
compare(IQ.A,IQ.A_SES,func = "PSIS") %>% round(2)
```

The model with 2 intercepts is clearly better because the difference between the models is larger than 2 sds.

## Linear regression with category-main effect and category-slope

Finally, we can also let the slopes vary by SES. We are estimating an **interaction model**. 

```{r class.source = 'fold-show'}
IQ.AxSES = 
  quap(
    alist(
      IQ ~ dnorm(mu,sigma),
      mu <- a[SES.c] + b[SES.c]*(A - 10),
      a[SES.c] ~ dnorm(100,5),
      b[SES.c] ~ dnorm(0,2),
      sigma ~ dexp(.5)
    ),
    data=dt)
# IQ ~ 0 + SES.c + A:SES.c
```

The key part of this model is that we are not specifying a main effect for A and an interaction effect with SES, but that we are **estimating 2 regression coefficients, one for high and one for low SES. This simplifies putting reasonable priors on the effect in each group, but also implies that we do not put a prior on the difference between the two groups.**


And again the posterior predictions: 

```{r fig.height=4, fig.width=4, fig.align = 'center', out.width="50%"}
par(mar=c(3,3,2,.5), mgp=c(1.75,.5,0), tck=-.01)
plot_data(dt)
plot_mu.CIs(IQ.AxSES, data.frame(A=A.seq, SES.c = 1), "black", spaghetti = TRUE)
plot_mu.CIs(IQ.AxSES, data.frame(A=A.seq, SES.c = 2), "red", spaghetti = TRUE)
```

Even though the DGP does not have different "intercepts" for high and low SES, we have to add it, because when the effect of A depends on SES, and childre with higher parental SES should have a higher IQ than children with lower parental SES and the same A.

Here is a comparison of all the models we have fit:

```{r class.source = 'fold-show'}
compare( IQ.A, IQ.A_SES, IQ.AxSES, func = "PSIS") %>% round(2)
```
```{r class.source = 'fold-show'}
compare(IQ.A, IQ.A_SES, IQ.AxSES, func = "WAIC") %>% round(2)
```

While the correct model has the best PSIS and WAIC value, for the top two models the differences are not large compared to the SEs of the differences.

### A simple contrast

How can we figure out if the difference in slopes is reliably larger than zero? We simply extract posteriors and calculate the difference in slopes from them:


```{r class.source = 'fold-show'}
# extract posterior
post = extract.samples(IQ.AxSES)
names(post)
head(post$b)
```

We simply calculate the differences of the two b parameters:

```{r class.source = 'fold-show'}
delta.b = post$b[,2]-post$b[,1]
```

And now we can show e.g. mean and PIs:

```{r class.source = 'fold-show'}
c(mean = mean(delta.b),
  PI(delta.b,prob = c(.9))) %>% 
  round(2)
```

And we can plot a histogram of the contrast:

```{r fig.height=3.5, fig.width=4, fig.align = 'center', out.width="50%"}
par(mar=c(3,3,2,.5), mgp=c(1.75,.5,0), tck=-.01)
hist(delta.b, xlim = range(c(0,delta.b)), main = "")
abline(v = 0, lty = 2)
abline(v = PI(delta.b,prob = c(.95)), col = "red")
```

### How about the DGP?

To see if our results are reasonable, lets compare our estimated parameters with the parameters governing the DGP. First the model parameters: 

```{r class.source = 'fold-show'}
precis(IQ.AxSES, depth = 2) %>% round(2)
```

Now the parameters from the DGP:

```{r class.source = 'fold-show'}
rbind(
  mean(b_A(SES[SES<quantile(SES,.30)])), 
  mean(b_A(SES[SES>quantile(SES,.70)])))
```

We are not recovering the exact parameters, after all we used a _golem_ instead of a model that depicts the DGP, but qualitatively the results are consistent with the DGP.

## Symmetric interactions

Earlier, we described the function

$$
IQ =  f(SES) A
$$

By saying that the effect $\small A$ is a function of $\small SES$. However, we really just have two variables: $\small A$ and $\small f(SES)$ which are multiplied with each other. Therefore, these statements are both true:

- The effect of $\small A$ depends on $\small f(SES)$
- The effect of $\small f(SES)$ depends on $\small A$

Accordingly, we can also plot the difference between high and low $\small SES$ as a function of $\small A$:

```{r fig.height=4, fig.width=8, fig.align = 'center'}
blue.25 = adjustcolor("blue", alpha = .25)
A.seq = seq(from=5,to=16,length.out=50)
mu.low = link(IQ.AxSES,data=data.frame(SES.c=1,A=A.seq))
mu.high = link(IQ.AxSES,data=data.frame(SES.c=2,A=A.seq))
delta = mu.high-mu.low
CIs = apply(delta,2,PI)
par(mfrow = c(1,2), mar=c(3,3,.5,.5), mgp=c(1.75,.5,0), tck=-.01)
plot(A.seq, colMeans(delta),'l',
     ylim = range(CIs),
     col = "blue",
     xlab = "A",
     ylab = expression(SES-effect: IQ[high~SES]~-~IQ[low~SES]))
shade(CIs,A.seq, col = blue.25)
abline(h = 0, lty = 2)


A.seq = c(10,12)
mu.low = link(IQ.AxSES,data=data.frame(SES.c=c(1,2),A=A.seq[1]))
mu.high = link(IQ.AxSES,data=data.frame(SES.c=c(1,2),A=A.seq[2]))
delta = mu.high-mu.low
CIs = apply(delta,2,PI)
plot(c(1,2), colMeans(delta), pch = 16, cex = 2,
     ylim = range(CIs), xlim = c(.8,2.2),
     col = "blue", xlab = "SES", xaxt = "n",
     ylab = expression(A-effect: IQ["A=12"]~-~IQ["A=10"]))
axis(1, at = c(1,2), labels = c("low","high"))
lines(c(1,1),CIs[,1], col = blue.25, lwd = 2)
lines(c(2,2),CIs[,2], col = blue.25, lwd = 2)
```

Nothing in the data determines which of the two points of view is more appropriate.

# Continuous interactions

We are continuing with our simulated Scarr-Rowe data set, but this time we use the full data and formulate a continuous interaction.

```{r}
dt = data.frame(
  A = A,
  IQ = IQ,
  SES = SES)
```

## Plotting the data

You can try it the fancy way and make a 3d plot, but in this instance its a lot of effort for meager results:

```{r fig.height=6, fig.width=6, fig.align = 'center', out.width="100%", warning = FALSE}
library(plot3D)
points3D(A,SES,IQ, type = "h",
         col = "black",
         cex = .75,
         lty = 3,
         pch = 16,
         phi = 20,
         theta = 45,
         xlab = "A",
         ylab = "SES",
         zlab = "IQ")
```

A panel of 2d plots (small multiples) does a better job, and will later allow to display uncertainty.

```{r fig.height=3, fig.width=10, fig.align = 'center', out.width="120%"}
qs = quantile(SES, probs = seq(0,1,.25))
par(mfrow = c(1,4), mar=c(2.5,2.5,2,.5), mgp=c(1.5,.5,0), tck=-.01)
for (k in 2:length(qs)) {
  idx = which(dt$SES > qs[k-1] & dt$SES < qs[k])
  tmp.dt = dt[idx,]
  with(tmp.dt,
       plot(IQ~A, pch = 16, main = paste0(k-1,". quartile SES"),
       ylim = range(dt$IQ), xlim = range(dt$A)))
}
  
```
How many panels one uses depends on how one assumes the moderator influences the effect of interest.

## Simple linear regression without interaction

This first model assumes that there are just main effects of $\small A$ and $\small SES$. Indeed, this is not an unreasonable assumption if one remembers the raw data, which we show here again:

```{r fig.height=4, fig.width=8, fig.align = 'center', out.width="60%", out.height="50%"}
par(mfrow = c(1,2),mar=c(3,3,1,.5), mgp=c(1.75,.5,0), tck=-.01)
plot(SES,IQ, pch = 16, cex = .5,)
plot(A,IQ, pch = 16, cex = .5,)
```


In preparation for the interaction model we are not centering $\small SES$, but we are re-scaling it to have a minimum just above 0. If we would center to zero, below zero SES values would predict a negative additive genetic effect.

The simple linear model can be descried as follows (omitting indicators $_i$ for individuals):

$$
\mu = \alpha + \beta_{A}A + \beta_{S}SES
$$

```{r class.source = 'fold-show'}
IQc.A_SES = 
  quap(
    alist(
      IQ ~ dnorm(mu,sigma),
      mu <- a + ba*(A - 6) + bs*(SES - 2),
      a ~ dnorm(100,10),
      ba ~ dnorm(0,4),
      bs ~ dnorm(0,4),
      sigma ~ dexp(.5)
    ),
    data=dt)
```

We are using prior predictions to check if the model does a good job:

```{r fig.height=3, fig.width=10, fig.align = 'center', out.width="120%"}
plot.pred(IQc.A_SES,dt, type = "prior")
```

This is pretty wild: Each line represents the expected IQ given A, which means that we should not see lines that lie mostly below or above the data points. Such lines indicate that the prior for the intercept should be adjusted, in our case it should be narrower.
Lets try a bit narrower priors:

```{r class.source = 'fold-show', fig.height=3, fig.width=10, fig.align = 'center', out.width="120%"}
set.seed(1)
IQc.A_SES = 
  quap(
    alist(
      IQ ~ dnorm(mu,sigma),
      mu <- a + ba*(A - 6) + bs*(SES - 2),
      a ~ dnorm(100,7.5),
      ba ~ dnorm(0,2),
      bs ~ dnorm(0,2),
      sigma ~ dexp(.5)
    ),
    data=dt)
plot.pred(IQc.A_SES,dt, type = "prior")
```

These priors are reasonable, so we look at the model-predictions:

```{r fig.height=3, fig.width=10, fig.align = 'center', out.width="120%"}
plot.pred(IQc.A_SES,dt)
```

As expected from the model we specified, all slopes are the same. Note that the different "intercepts" we see come from this part of the model: `mu <- ... + bs*(SES - 2) + ...`.

## Linear regression with main effects and interaction

If we want to model an interaction effect, we want to model that the effects of $\small A$ and $\small SES$ depend on each other. Broadly, we want to implement 

$$
\gamma_A = f(SES)
$$

i.e., the effect of $\small A$ is a function of $\small SES$. If we assume that $\small f(SES)$ is a linear function, we are saying that

<div class="marginnote"> 

```{r, fig.align="left", fig.width=4, fig.height=4}
curve(b_A(x),0,10, 
      ylab = expression("effect size"~beta[A]~" = f(SES)"), 
      ylim = c(0,b_A(10)), xlab = "SES")
hist(SES,add = T, probability = T)
```
</div>

$$
\gamma_A = \beta_A + \beta_{AS}SES
$$

Here, $\small \beta_A$ is the intercept for the effect of $\small A$, i.e. the effect of $\small A$ when $\small SES = 0$. If we knew that the effect of $\small A$ has to be zero when $\small SES = 0$, we could omit the $\small \beta_A$

Now lets think back to our original regression, with slightly modified notations

$$
\mu = \alpha + \color{Red}{\gamma_{A}}A + \beta_{S}SES
$$

We can just plug in $\small (\beta_A + \beta_{AS}SES)$ in place of $\small \gamma_a$:

$$
\mu = \alpha + \color{Red}{(\beta_A + \beta_{AS}SES)}A + \beta_{S}SES
$$

and if we multiply out the brackets, we get

$$
\mu = \alpha + \color{Red}{\beta_A A + \beta_{AS}SES \, A}+ \beta_{S}SES 
$$

which is the standard interaction model with 2 main effects.

The `quap` model is then similar to the previous, but adds this interaction term: `bas*(A - 6)*(SES - 2)`.

```{r fig.height=3, fig.width=10, fig.align = 'center', out.width="120%", class.source = 'fold-show'}
IQc.AxSES.m = 
  quap(
    alist(
      IQ ~ dnorm(mu,sigma),
      mu <- a + ba*(A - 6) + bs*(SES - 2) + bas*(A - 6)*(SES - 2),
      a ~ dnorm(100,7.5),
      ba ~ dnorm(0,2),
      bs ~ dnorm(0,2),
      bas ~ dnorm(0,1),
      sigma ~ dexp(.5)
    ),
    data=dt)
# IQ ~ A + SES + A:SES
plot.pred(IQc.AxSES.m,dt, type = "prior")
```

This priors look OK (even though slopes become extreme at high SES values), so we plot the model predictions:

```{r fig.height=3, fig.width=10, fig.align = 'center', out.width="120%"}
plot.pred(IQc.AxSES.m,dt)
```

We now clearly see that the slope, the effect of A, is steeper when SES is higher.

## Model comparison

```{r class.source = 'fold-show'}
compare(IQc.A_SES,IQc.AxSES.m, func = "WAIC") %>% round(1)
```

```{r class.source = 'fold-show'}
compare(IQc.A_SES,IQc.AxSES.m, func = "PSIS") %>% round(1)
```

PSIS and WAIC correctly identify the interaction model, and the difference is clearer compared to the categorical interaction, which used only a subset of the data and through way information by dichotomizing.

## A fancy plot
```{r fig.align = 'center', warning=FALSE, message=FALSE}
library(akima)
library(plotly)
mu = link(IQc.AxSES.m)
s = interp(dt$A,dt$SES,colMeans(mu))
names(s) = c("A","SES","IQ")
fig = with(
  s,
  plot_ly(x = ~A, y = ~SES, z = ~IQ,
          width = 900, height = 900) %>% 
    add_surface(
      contours = list(
        z = list(
          show=TRUE,
          usecolormap=TRUE,
          highlightcolor="#ff0000",
          project=list(z=TRUE)
        )
      )
   )%>% 
    add_markers(x = dt$A, y = dt$SES, z = dt$IQ, size = .5)
)
fig = 
  fig %>% layout(
    scene = list(
      camera=list(
        eye = list(x=1.87, y=0.88, z=0.64)
      )
    )
  )

fig
```

# Scarr-Rowe effect: Latest results

![Giagrande 2019: Scarr-Rowe effect](Giagrande_ScarrRoweEffect.png)

# Questions

- How do you make the three diagrams next to each other to investigate interaction effects with centralized and extreme values? The book doesnâ€™t show this clearly. 
- What is the difference between moderating and interacting effects?

# Exercises

## E1
Which of the following explanations invokes an interaction?

No interaction here. We need some type of "and" statement for an interaction.

> Intelligent animal species tend to be either highly social or have manipulative appendages (hands, tentacles, etc.).

## M1
Can you explain this result in terms of interactions between water, shade, and temperature?

> Recall the tulips example from the chapter. Suppose another set of treatments adjusted the temperature in the greenhouse over two levels: cold and hot. The data in the chapter were collected at the cold temperature. You find none of the plants grown under the hot temperature developed any blooms at all, regardless of the water and shade levels. Can you explain this result in terms of interactions between water, shade, and temperature?

This is indeed a three way interaction. A quick way would be to simply write the model as 

$$
mu_i = \alpha + \beta_{TSW}(T_i \cdot S_i \cdot W_i)
$$

However, it is a good idea to generally model all lower level main and interaction effects

$$
\mu_i = \\
\alpha + \beta_{T}T_i + \beta_{S}S_i + \beta_{W}W_i \\
+ \beta_{TS}(T_i \cdot S_i) + \beta_{SW}(S_i \cdot W_i)+  \beta_{TW}(T_i \cdot W_i) \\
+ \beta_{TSW}(T_i \cdot S_i \cdot W_i)
$$


## M3
Can you invent a regression equation that would make the bloom size zero, whenever the temperature is hot?

The trick here is that we need 
- a binary indicator variable, for hotness, lets call it $\small H$ that is $0$ when it is hot and otherwise one $1$.
- an equation where all other effects interact with hotness.

Here is an example that works:

$$
\mu_i = (\alpha + \beta_{T}T_i + \beta_{S}S_i + \beta_{W}W_i) \cdot H_i
$$

Here is an example that does not work:

$$
\mu_i = \alpha + \beta_{T}T_i + \beta_{S}S_i + \beta_{W}W_i\cdot H_i
$$

If we have any terms in the model that are not multiplied by hotness, we cannot insure that $\mu$ will be zero when it is hot. _This is also true for a model where the intercept depends on hotness_, for example 

$$
\mu_i = \alpha_{[H_i]} + \beta_{T}T_i + \beta_{S}S_i + \beta_{W}W_i
$$

will not work because $\small \beta_{T}T_i + \beta_{S}S_i + \beta_{W}W_i$ can be non-zero when it is hot.


## M4

Repeat the tulips analysis, but this time use priors that constrain the effect of water to be positive and the effect of shade to be negative. Use prior predictive simulation. What do these prior assumptions mean for the interaction prior, if anything?.

### Forcing a negative prior

```{r}
data(tulips)
d <- tulips
d$blooms_std <- d$blooms / max(d$blooms)
d$water_cent <- d$water - mean(d$water)
d$shade_cent <- d$shade - mean(d$shade)
```


If we have a model `mu <- a + bw*w + bs*s` and want to force a positive or negative parameter, **we generally start by choosing a prior distribution that has only positive values**, e.g. `dlnorm` or `dexp`. This is sufficient for positive parameters. For negative paramters we also have to change the sign in the model for mu: `mu <- a + bw*w - bs*s`

The a full quap model could be described as:

```{r class.source = 'fold-show'}
m = quap(
  alist(
    blooms_std ~ dnorm(mu, sigma),
    mu <- a + bw*water_cent - bs*shade_cent + bws*water_cent*shade_cent,
    a ~ dnorm(0.5, 0.25),
    bw ~ dlnorm(0, 0.25),
    bs ~ dlnorm(0, 0.25),
    bws ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), data=d )
```

### Adjusting prior parameters

Here are the prior predictions:

```{r, fig.width=9, fig.height=3}
plot_prior_predictive = function(m) {
  p = extract.prior(m) 
  par(mfrow=c(1,3),mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01) # 3 plots in 1 row
  for ( s in -1:1){
    idx <- which( d$shade_cent==s )
    plot( d$water_cent[idx], d$blooms_std[idx], xlim=c(-1,1), ylim=c(0,1),
          xlab="water", ylab="blooms")
    mtext( concat( "shade = ", s))
    mu = link( m, post=p, data=data.frame( shade_cent=s, water_cent=-1:1))
    for ( i in 1:50) lines( -1:1, mu[i,], col=col.alpha("blue",0.15) )
  }
}
plot_prior_predictive(m)
```

This is a bit wild. We see that the effect of water is much to steep, so we give the prior for the slopes a lower mean.


```{r  class.source = 'fold-show', fig.width=9, fig.height=3}
m = quap(
  alist(
    blooms_std ~ dnorm(mu, sigma),
    mu <- a + bw*water_cent - bs*shade_cent + bws*water_cent*shade_cent,
    a ~ dnorm(0.5, 0.25),
    bw ~ dlnorm(-2, 0.25),
    bs ~ dlnorm(-2, 0.25),
    bws ~ dnorm(0, 1),
    sigma ~ dexp( 1 )
  ), data=d )
```

<div class="marginnote"> 
There is no quick and easy solution to find the right prior here. Here is one way to go about: At shade = 0 bloom ranges from around 0 to around .6 when water ranges from -1 to 1. So the maximum slope is around `.6/.2` = `r .6/.2`. That is, we are looking for a log-normal distribution where values > 0.3 are rare (the 97.5% percentile should be around 0.3). The quickest way to get this can then be to use either `hist(rlnorm(10000,mu,sd))` or `curve(dlnorm(x,mu,sd))` and play around with different values for `mu` and `sd` until one gets the desired distribution.
</div>

```{r echo=FALSE}
plot_prior_predictive(m)
```

 
The slopes at shade=1 are now OK (could be a but more variable), but we also see that the effect of water can be positive or negative when there is no shade (shade = -1) or a lot of shade (shade 1).
But when there is little shade, more water should have a positive effect. If we look at our linear model again:

`mu <- a + bw*water_cent - bs*shade_cent + bws*water_cent*shade_cent`

the term `bws*water_cent*shade_cent` should be positive if shade is -1. To simplify things, lets assume there is a lot of water, so we want that `(bws * 1 * -1) > 0` ,which we only get if we enforce that bws is a negative number. We achieve this by also putting a lognormal prior on it and by subtracting this term, instead of addining it.

`mu <- a + bw*water_cent - bs*shade_cent + bws*water_cent*shade_cent,` <br> `...` <br> `bws ~ dnorm(0, 1)`

becomes 

`mu <- a + bw*water_cent - bs*shade_cent - bws*water_cent*shade_cent` <br> `...` <br> `bws ~ dlnorm(-2, .25)`

Here is the full model with prior predictive simulations:

```{r class.source = 'fold-show', fig.width=9, fig.height=3}
m = quap(
  alist(
    blooms_std ~ dnorm(mu, sigma),
    mu <- a + bw*water_cent - bs*shade_cent - bws*water_cent*shade_cent,
    a ~ dnorm(0.5, 0.25),
    bw ~ dlnorm(-2, 0.25),
    bs ~ dlnorm(-2, 0.25),
    bws ~ dlnorm(-2, 0.25),
    sigma ~ dexp(1)
  ), data=d )
plot_prior_predictive(m)
```


Lastly, we see that blooming is to high when shade = 0. If we look again at the linear model `mu <- a + bw*water_cent - bs*shade_cent - bws*water_cent*shade_cent` we see that we need to adjust the intercept to deal with this. Our final model is then:

```{r class.source = 'fold-show', fig.width=9, fig.height=3}
m = quap(
  alist(
    blooms_std ~ dnorm(mu, sigma),
    mu <- a + bw*water_cent - bs*shade_cent - bws*water_cent*shade_cent,
    a ~ dnorm(0.3, 0.25),
    bw ~ dlnorm(-2, 0.25),
    bs ~ dlnorm(-2, 0.25),
    bws ~ dlnorm(-2, 0.25),
    sigma ~ dexp(1)
  ), data=d )
plot_prior_predictive(m)
```

