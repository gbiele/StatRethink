---
title: "Chapter 11: Recap"
author: "Guido Biele"
date: "25.04.2022"
output:
  html_document: 
    mathjax: default
    toc: true
    toc_depth: 2
    code_folding: hide
header-includes: 
    \usepackage{xcolor}
    \usepackage{amsmath}
---

  
```{css, echo = F}
body{
  font-family: Helvetica;
  font-size: 16pt;
  max-width: 1000px;
  margin: auto;
  margin-left:310px;
}
pre{
  font-size: 20px;
}
/* Headers */
h1{
    font-size: 24pt;
  }
h1,h2{
    font-size: 20pt;
  }
h3,h4,h5,h6{
  font-size: 18pt;
}

#TOC {
  position: fixed;
  left: 0;
  top: 0;
  width: 300px;
  height: 100%;
  overflow:auto;
}

.sidenote, .marginnote { 
  float: right;
  clear: right;
  margin-right: -50%;
  width: 40%;         # best between 50% and 60%
  margin-top: 0;
  margin-bottom: 0;
  line-height: 2;
  font-size: 1.8rem;
  vertical-align: baseline;
  position: relative;
  }
```

```{r setup, include=FALSE, message=FALSE, warning=FALSE, results='hide'}
knitr::opts_chunk$set(echo = TRUE, dpi = 300, global.par = TRUE,
                      fig.align = 'center', out.width="80%")
library(rethinking)
library(magrittr)
library(knitr)
library(kableExtra)
library(MASS)
source("../utils.R")
options(mc.cores = 4)
cmdstanr::set_cmdstan_path("C:/Users/gubi/cmdstan-2.32.0/")

set_par = function(mfrow = c(1,1), mar=c(3,3,.5,.5), cex = 1) 
  par(mfrow = mfrow, mar=mar, mgp=c(1.75,.5,0), tck=-.01, cex = cex)
```

# Generalized linear models

So far we have used linear regressions, where we modeled the outcome as follows:

$$
y \sim Normal(\mu,sigma) \\
\mu  = \alpha + \beta X
$$

This regression works generally well, even if $\small y$ is not normally distributed. (The residuals should be though. And a posterior predictive plot is always useful!)

## Distributions

However, for some outcome types a linear regression simply does not work. This is particularity clear when the outcome is bound to be equal to or larger than zero (like counts that "clump" at zero) or when the outcome is binary.

As an example the next figure shows end of year math grades in a Portuguese school.^[P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7. The data can be downloaded [here](https://archive.ics.uci.edu/ml/datasets/Student+Performance)]

```{r out.width="80%", fig.height=4}
df=read.table("data/student-mat.csv",sep=";",header=TRUE)
df = df[df$Medu>0,]
set_par()
x = barplot(c(table(df$G3),0,0,0), xaxt = "n", border = "grey")
axis(1,at = x[seq(0,20,2)+1], labels = seq(0,20,2))
```


In these situations we can use this kind of model:

$$
y \sim dist(\theta_1,\theta_2) \\
$$

Here $\small dist(\theta_1,\theta_2)$ is a distribution with two parameters^[There are also distributions with 1 or 3 or more parameters] that is consistent with the observed data.

__Choosing a different distribution means choosing a different likelihood function__. That is, we exchange `dnorm` with an alternative distribution that is appropriate for the data.

Here are a few examples:

- The [Binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution) models the _"number of successes in a sequence of n independent experiments, each asking a yesâ€“no question"_. Hence we use the Binomial distribution function to calculate the likelihood when we model the number of successes. A special case is when we have only one trial / experiment, then the Binomial distribution models binary outcomes.

```{r out.width="80%", fig.height=4}
set_par()
x = seq(0,10,1)
plot(x-.2,dbinom(x, 10, .1),"h", xlim = c(0,20), lwd = 2,
     ylab = "probability mass", xlab = "number of successes")
x = seq(0,10,1)
lines(x,dbinom(x, 10, .5),"h", col = "blue", lwd = 2)
x = seq(0,20,1)
lines(x+.2,dbinom(x, 20, .75),"h", col = "red", lwd = 2)
legend("topright",
       lwd = 2, col = c("black","blue","red"),
       legend = c(
         expression(theta[1]~" = n = 10, "~theta[2]~" = p = .1"),
         expression(theta[1]~" = n = 10, "~theta[2]~" = p = .5"),
         expression(theta[1]~" = n = 20, "~theta[2]~" = p = .75")
       ),
       bty = "n")
```

- The [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) _"expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate"_ . Hence we use the Poisson distribution function to calculate the likelihood when we model the occurrence of events (counts).

```{r, out.width="80%", fig.height=4}
set_par()
x = seq(0,5,1)
plot(x-.2,dpois(x, .5),"h", xlim = c(0,20), lwd = 2,
     ylab = "probability mass", xlab = "number of events")
x = seq(0,10,1)
lines(x,dpois(x, 5),"h", col = "blue", lwd = 2)
x = seq(0,20,1)
lines(x+.2,dpois(x, 10),"h", col = "red", lwd = 2)
legend("topright",
       lwd = 2, col = c("black","blue","red"),
       legend = c(
         expression(theta[1]~" = "~lambda~" = .5"),
         expression(theta[1]~" = "~lambda~" = 5"),
         expression(theta[1]~" = "~lambda~" = 10")
       ),
       bty = "n")
```

## Link functions

As can be seen from the previous plots, the expected value (the mean parameter) of the Binomial and Poisson distributions are constrained:

- For the Binomial distribution:  $\small 0 \leq \textrm{success probability } p \leq 1$
- For the Poisson distribution:  $\small 0 < \textrm{expected rate } \lambda$.

But if we would just model e.g. 
$$
p =  \alpha + \beta X
$$
we could get values between minus and plus infinity. Therefore, the generalized linear model also uses link functions that map the result of the _linear predictor_ $\small \alpha + \beta X$ to the desired range:

$$
p = \textrm{inv.link}(\alpha + \beta X) \\
\textrm{or}\\
\textrm{link}(p) = \alpha + \beta X
$$

GLMs use different link functions for different distributions to implement different constraints:

The _classical_ link function for the binomial distribution is based on the logit function:

$$
\textrm{logit}(p) = log(\frac{p}{1-p})
$$

where $\frac{p}{1-p}$ are odds, i.e. the ratio of two probabilities.

The _classical_ link function for the Binomial distribution is then $\small p = inv.logit(\alpha + \beta X)$

$$
inv.logit(x) = \frac{exp(x)}{1+exp(x)} = \frac{1}{1+exp(-x)}
$$

```{r, out.width="80%", fig.height=4, fig.width=8}
set_par()
par(mfrow = c(1,2))

inv.logit = function(x) 1/(1+exp(-x))
log.odds = function(x) {p = inv.logit(x); return(log(p/(1-p)))}
  
curve(log.odds(x),-4.1,4.1,n = 100,
      xlab = expression(alpha~" + "~beta~"X"), ylab = "log odds")
  
curve(inv.logit(x),-4.1,4.1,n = 100,
      xlab = expression(alpha~" + "~beta~"X"), ylab = "p")
curve(pnorm(x),add = T, col = "grey", lty = 2)
```


- For the Poisson: $\lambda = exp(\alpha + \beta X)$

```{r, out.width="40%", fig.height=4, fig.width=4}
set_par()
curve(exp(x),-3,4,n = 100,
      xlab = expression(alpha~" + "~beta~"X"), ylab = expression(lambda))
```

**One important effect of link functions is that we cannot interpret regression weights in the same way as for simple linear regression.** In a nutshell, if link functions exponentiate the linear predictor, regression weights represent multiplicative rather than additive effects.

In `R`, `family` is the term for an object that tells a regression model both what outcome distribution to use and what link function to use.

When do we use the Binomial and when the Poisson likelihood?

<div class="marginnote"> 
People are "produced" at a constant rate and psychologists is just a fraction of people.
</div>
- Poisson: When things are produced at a constant rate and there is no clear limit to the number of produced things
  - number of harvested apples in an orchard
  - number of people with a psychology degree
  - number of tests taken

- Binomial: When the counted number of success is limited by the number of trials. This is the reason that the Binomial distribution has 2 parameters.
  - number of bad apples *among* all harvested apples
  - number of PhDs *among* psychologists
  - number of failed tests *out of* all (could be one) tests
  
  
  
# Logistic regression

As a hands on example for logistic regression we will try to predict failing the math class in the Portuguese school data shown above.
In particular, we will use these predictors:

- maternal education
- numbers of times the class was failed previously

It is always a good idea to plot the data first:

```{r out.width="80%", fig.height=3.5}
data.list = list(
  Medu = df$Medu,
  failures = df$failures,
  fail = df$G3 == 0
) 
set_par(mfrow = c(1,2))
table(data.list$failures) %>% barplot(border = "grey", ylab = "count", xlab = "number previous fails")
table(data.list$Medu) %>% barplot(border = "grey", xlab = "maternal education")
```

We are centering `Medu`, so that the intercept measures odds at "middle high" maternal education.

```{r class.source = 'fold-show'}
data.list$cMedu = data.list$Medu-2.5
```

We begin the analysis with an intercept model only. Especially if one is not familiar with a model, it is good to start simple and add complexity step by step.

```{r class.source = 'fold-show'}
model = alist(
  fail ~ dbinom(1,p),
  logit(p) <- a,
  a ~ dnorm(0,10)  # note the wide prior
)
```

## Prior predictive check

And we estimate the model with `ulam` (i.e. Stan):


```{r, class.source = 'fold-show', warning=FALSE, message=FALSE, results='hide'}
# If we only want prior predictions, it is faster to use quap.
u.fit_I = quap(
  model,
  data = data.list) 
```


We extract the prior and plot the prior prediction for the failure rate:

```{r class.source = 'fold-show', fig.width=4, fig.height=3.5,  warning=FALSE, message=FALSE, results='hide', out.width="50%"}
prior = extract.prior(u.fit_I)
set_par()
hist(inv_logit(prior$a), main="", breaks = 30)
```

To see what is going on here, lets overlay the logistic function on the prior:

```{r, out.width="100%", fig.height=4, fig.width=8}
set_par(mfrow = c(1,2), mar=c(3,3,3,3), cex = 1)

curve(dnorm(x,0,10),-30,30, ylab = "prior density", xlab = "a")
axis(4,at = seq(0,0.04, length.out = 5),col.axis = "red",
     labels = seq(0,1,length.out = 5), col = "red")
curve(inv_logit(x)/25, add = TRUE, col = "red")
abline(h = .1/25, lty = 3, col = "grey")
mtext(side = 4, "p = inv.logit(a)", line = 1.5, col = "red")
title("a ~ dnorm(0,10)")
rect(-50,-1,-5,1, col = adjustcolor("blue", alpha = .25), lty = 0)
rect(5,-1,50,1, col = adjustcolor("blue", alpha = .25), lty = 0)

curve(dnorm(x,-1,1),-5.1,5.1, ylab = "prior density", xlab = "a")
axis(4,at = seq(0,0.4, length.out = 5), col.axis = "red",
     labels = seq(0,1,length.out = 5), col = "red")
curve(inv_logit(x-1)/2.5, add = TRUE, col = "red")
abline(h = .1/5, lty = 3, col = "grey")
mtext(side = 4, "p = inv.logit(a)", line = 1.5, col = "red")
title("a ~ dnorm(-1,1)")
rect(-50,-1,-5,1, col = adjustcolor("blue", alpha = .25), lty = 0)
rect(5,-1,50,1, col = adjustcolor("blue", alpha = .25), lty = 0)
```

With a wide prior, most of the probability mass is either smaller than -5, leading to $p$ very close to 0, or larger than 5, leading to $p$ close to 1. Hence we will use a tighter prior on a. In addition, we know the failure proportion is closer to zero than to one, so we can set the mean to a number below 0.

Here is the prior distribution of p for a N(-1,1) prior on a:

```{r class.source = 'fold-show', fig.height=4, fig.width=4, out.width="50%"}
set_par()
rnorm(10000,-1,1) %>% 
  inv_logit() %>% 
  hist(breaks = seq(0,1,length.out = 25), main = "")
title("inv_logit(rnorm(1000,-1,1))", cex.main=1)
```


How about the regression coefficients $\small \beta$? Regression coefficients are log odds ratios.

We explain odd ratios (OR) with the following example:

- for children who _never_ failed a class, 2 out of 100 children fail the class now
- for children who _once_ failed a class, 4 out of 100 children fail the class now

then the odds ratio is :

$$
\begin{align*}
OR = & \frac{\textrm{failure odds with no prior fail}}{\textrm{failure odds with one prior fail}} \\
= & \frac{\frac{2}{98}}{\frac{4}{96}} = \frac{.0204}{.0417} = .49
\end{align*}
$$


> Note that this specific odds ratio comes close to the risk ratio of $\small .2 / .4 = .5$.
> This is, however, not generally the case. If the probabilities are far away from zero, risk ratio and odds ratio do not align! We can see this if we just multiply the probability of failure by 15 in both groups:
> $$
> \begin{align*}
> OR = \frac{\frac{30}{70}}{\frac{60}{40}} = \frac{.428}{.667} = .29
> \end{align*}
> $$


Lets get back to specifying our prior for $\small \beta$:

- We had an examplary odds ratio of 0.49, which corresponds to a log odds ratio of log(.49) ~ -.7. 
- This means that if we assume that fail-probability is low and a shift from no to one prior fail comes with a doubling of the fail probability, we should comfortably allow $\small \beta$ values of size .7.
- If we wanted to set an informative shrinkage prior, which however does not prefer one direction of the effect, we could set a `dnorm(0,.7)` prior.
- But we are not so sure and don't like too informative priors, so we set a `dnorm(0,2)` prior.

Lets specify such a model and look at the prior predictions for the difference between two levels of education.


```{r class.source = 'fold-show', warning=FALSE, message=FALSE, results='hide'}
model = alist(
  fail ~ dbinom(1,p),
  logit(p) <- a + b1*failures,
  a ~ dnorm(-1,1),
  b1 ~ dnorm(0,2)
)
u.fit_F = quap(
  model,
  data = data.list)  

prior = extract.prior(quap(model, data = data.list))
```

Now we can use the `link` function and new data to generate predictions from the prior: 

```{r class.source = 'fold-show', out.width="100%", fig.height = 4, fig.width=8}
# simultate outcomes from the prior
p = link(
  u.fit_F,                        # the model fit
  post = prior,                   # we want simulations from the prior
  data = list(failures = c(0,1))) # the data used for the simulation
set_par(mfrow = c(1,2), mar=c(3,3,3,3), cex = 1)
hist(p[,2]-p[,1],
     main = "risk difference", xlab = "P(fail|high) - P(fail|low)", )
hist(p[,2]/p[,1], breaks = 30,
     main = "risk ratio", xlab = "P(fail|high) / P(fail|low)")
```

These differences and ratios are pretty large, so it is safe to make the prior for `b1` a bit narrower and set the standard deviation for the regression weights to 1. [It is not surprising or wrong that positive differences are more likely, which is due to the prior on `a`].

Which predictions one generates from the prior depends on research questions and domain knowledge. My approach is to check if prior-predicted data are broadly consistent with the overall distribution of the data and to check if effect estimates fall into a plausible range. The latter is easier for Gaussian models and harder for models with exponential link functions. 

One **simple rule of thumb for logistic or binomial regressions priors** is that with a $\small Normal(0,1)$ prior on regression coefficients the 97.5 percentile is 1.96. If we exponentiate this we get 7, which means that a $\small Normal(0,1)$ prior expresses the prior information that the OR is probably not much larger then 7 or smaller than 1/7. Note that this prior would still be overwhelmed if we have a reasonable amount (N>25)^[I plugged this number from thin air, should really to a quick calculation on this.] of data.

Here is the our model so far, which include previous class fails and maternal education as predictors: 

```{r  class.source = 'fold-show', warning=FALSE, message=FALSE, results='hide'}
model = alist(
  fail ~ dbinom(1,p),
  logit(p) <- a + b1*failures + b2*cMedu,
  a ~ dnorm(-1,1),
  b1 ~ dnorm(0,1),
  b2 ~ dnorm(0,1)
)
u.fit_FE = ulam(
  model,
  data = data.list,
  log_lik = TRUE,   # for model comparison
  iter = 2000,      # 2000 iterations, (1000 warmup)
  chains = 4,       # four chains, to check convergence
  cores = 4,        # use 4 cores in parallel
  cmdstan = TRUE)   # use cmdstanr not rstan
```

Some quick convergence diagnostics:

```{r}
precis(u.fit_FE, depth = 2)
```


## Posterior predictive checks

To see if our model describes the data well, we plot model predicted and observed data together:

```{r, out.width="85%", fig.height=5, fig.width = 5, fig.cap="Observed (gray) and predicted proportion of fails by maternal education"}
aggr.fun = function(x) return(c(mean = mean(x), N = length(x)))
obs = 
  aggregate(
  data.list$fail,
  by = with(data.list, 
            data.frame(cMedu = cMedu, Medu = Medu, failures = failures)), 
  aggr.fun
  )
obs = cbind(obs,obs$x)

sim.data = 
  unique(as.data.frame(data.list)[,c(2,4)])

p = link_ulam(
  u.fit_FE,
  data = sim.data)

pp.stats = cbind(
  sim.data,
  m = colMeans(p),
  t(apply(p,2,PI))
)

set_par(mfrow = c(2,2), mar=c(3,3,1.5,0.5), cex = .75)
tmp = 
  lapply(unique(obs$cMedu), function(x) {
  plot(obs[obs$cMedu == x,"failures"],
       obs[obs$cMedu == x,"mean"],
       cex = sqrt(obs[obs$cMedu == x,"N"]),
       xlim = c(-.5,3.5), ylim = c(0,.5),
       ylab = "proportion fail",
       xlab = "# past failures",
       xaxt = "n", pch = 16, col = "grey")
    title(paste("maternal edu",x+2.5),line = 0.5, cex.main = 1)
    axis(1,at = 0:3)
    points(pp.stats[pp.stats$cMedu == x,"failures"],
       pp.stats[pp.stats$cMedu == x,"m"], pch = 16, col = "blue")
    arrows(pp.stats[pp.stats$cMedu == x,"failures"],
           y0 = pp.stats[pp.stats$cMedu == x,"5%"],
           y1 = pp.stats[pp.stats$cMedu == x,"94%"],
           length = 0, col = "blue")
})
```

This does not look great. We can try to improve the model in two ways:

- give each level of maternal education its own intercept
- give each level of maternal education its own slope

We are using the indexing approach discussed in the book for this.

```{r class.source = 'fold-show', warning=FALSE, message=FALSE, results='hide'}
model = alist(
  fail ~ dbinom(1,p),
  logit(p) <- a[Medu] + b[Medu]*failures,
  a[Medu] ~ dnorm(-1,1),
  b[Medu] ~ dnorm(0,1)
)
u.fit_FE.2 = ulam(
  model,
  data = data.list,
  log_lik = TRUE,   # for model comparison
  iter = 2000,      # 2000 iterations, (1000 warmup)
  chains = 4,       # four chains, to check convergence
  cores = 4,        # use 4 cores in parallel
  cmdstan = TRUE)   # use cmdstanr not rstan
```

Did the chains converge?

```{r class.source = 'fold-show'}
precis(u.fit_FE.2, depth = 2) %>% round(2)
```

This looks OK, so we do again a posterior predictive check:

```{r, out.width="100%", fig.height=6}
sim.data = 
  unique(as.data.frame(data.list)[,c(1,2)]) 

p = link_ulam(
  u.fit_FE.2,
  data = sim.data)

pp.stats = cbind(
  sim.data,
  m = colMeans(p),
  t(apply(p,2,PI))
)

set_par(mfrow = c(2,2), mar=c(3,3,1.5,0.5), cex = .75)
tmp = 
  lapply(unique(obs$Medu), function(x) {
  plot(obs[obs$Medu == x,"failures"],
       obs[obs$Medu == x,"mean"],
       cex = sqrt(obs[obs$Medu == x,"N"]),
       xlim = c(-.5,3.5), ylim = c(0,.5),
       ylab = "proportion fail",
       xlab = "# past failures",
       xaxt = "n", pch = 16, col = "grey")
    title(paste("maternal edu",x),line = 0.5, cex.main = 1)
    axis(1,at = 0:3)
    points(pp.stats[pp.stats$Medu == x,"failures"],
       pp.stats[pp.stats$Medu == x,"m"], pch = 16, col = "blue")
    arrows(pp.stats[pp.stats$Medu == x,"failures"],
           y0 = pp.stats[pp.stats$Medu == x,"5%"],
           y1 = pp.stats[pp.stats$Medu == x,"94%"],
           length = 0, col = "blue")
})
```


This does not look that much better. Let's check this with PSIS-LOO:

```{r class.source = 'fold-show'}
compare(u.fit_FE.2,u.fit_FE, func = "PSIS")
```

In this case the added complexity has not helped much (most of the data is at 0 past failures). So lets look at the parameters of the first model:

```{r class.source = 'fold-show', out.height="50%"}
plot(precis(u.fit_FE, depth = 2))
precis(u.fit_FE, depth = 2) %>% round(2)
```

```{r, echo = FALSE}
summary = precis(u.fit_FE, depth = 2) %>% round(2)
summary = cbind(p = rownames(summary), summary)
a = summary$mean[summary$p == "a"]
b1 = summary$mean[summary$p == "b1"]
b2 = summary$mean[summary$p == "b2"]
```


What does this all mean?

Because the parameters are log-odds ratios, we exponentiate them to get ORs. Because the probability of our outcome is (relatively) close to zero, we can interpret the OR as approximate risk ratios. Hence

- the intercept value is `r a`, which gives a baseline risk of `r round(exp(a),2)` to fail.
- the regression weight for prior failures is `r b1`, which means that the odds ratio and approximate relative risk to fail the class for one more prior failed class is exp(`r b1`) = `r round(exp(b1),2)`. 
- the regression weight for maternal education is `r b2`, which means that the odds ratio and approximate relative risk to fail the class if a mother has a one level higher education is exp(`r b2`) = `r round(exp(b2),2)`. Correspondingly, if maternal education is changed by two levels, the OR (~RR) is exp(2 * `r b2`) = `r round(exp(2*b2),2)`.

## Constrasts

The OR and relative risks, which we have to deal with due to the link function, make it hard to interpret the results. But if we create posterior predictions we can simply calculate contrasts. i.e. risk differences.

Lets look at **unique patterns** the data we used to generate posterior predictions:

```{r class.source = 'fold-show'}
sim.data = 
  unique(as.data.frame(u.fit_FE@data)[, c("cMedu","failures","Medu")]) 
sim.data = sim.data[order(sim.data$cMedu,sim.data$failures),]
rownames(sim.data) = NULL
sim.data
```

If we now generate the **posterior predictions for unique patterns**, we get a matrix in which each column corresponds to one row in `sim.data`

```{r class.source = 'fold-show'}
pp = link_ulam(
  u.fit_FE,
  data = sim.data)
dim(pp)
```

A simple question would be, what the effect of having one vs zero previous failures is when maternal educational level is 1. We simply look up in the `sim.data` table that we need to compare columns one and two in the posterior predictions for this. In the same manner we can do this for maternal educational level 2.

```{r class.source = 'fold-show'}
# effect of prior failure for Medu = 1
delta.L1 = pp[,2] - pp[, 1]
# effect of prior failure for Medu = 2
delta.L2 = pp[,6] - pp[, 5]
precis(
  list(delta.L1 = pp[,2] - pp[, 1],
       delta.L2 = pp[,6] - pp[, 5],
       delta.diff = pp[,6] - pp[, 5] - pp[,2] + pp[, 1])) 
```

```{r, fig.height=4, fig.width=12, out.width="125%"}
# plot data
xlim = range(c(delta.L1,delta.L2))
xlim[1] = ifelse(xlim[1]>0,0,xlim[1])
breaks = seq(xlim[1]-.01,xlim[2]+.01, length.out = 25)
set_par(mfrow = c(1,3), cex = 1.5)
hist(delta.L1, main = "", xlim = xlim, breaks = breaks)
abline(v = 0, col = "red", lty = 3)
hist(delta.L2, main = "", xlim = xlim, breaks = breaks)
abline(v = 0, col = "red", lty = 3)
hist(delta.L2-delta.L1, main = "")
abline(v = 0, col = "red", lty = 3)
```

**The right histogram shows an interaction effect on the scale of risk differences, even though our model has no interaction term!** This is due to the exponent in the link function!

> This interaction effect can also be understood by remembering that odds ratios are relative effects, i.e. the effect of a one unit change of a variable depends on the probability of success at the comparison value. Here is an example with numbers. Lets assume our intercepts for two groups are 1 and 2, respectively, and the log-odds ratio for the effect of interest is 1. Then we can calculate the risk differences as
> - `inv_logit(1+1) - inv_logit(1+0)` = `r round(inv_logit(2) - inv_logit(1),2)` 
> - and `inv_logit(2+1) - inv_logit(2+0)` = `r round(inv_logit(3) - inv_logit(2),2)`
> and see that the absolute effect of the logg odds ratio depends on the baseline odds. Calculate `inv_logit(1+1) / inv_logit(1+0)` and `inv_logit(2+1) / inv_logit(2+0)` and compare the results.
> Therefore, the meaningfulness of a result in log odds ratios can only be understood based on some background knowledge.
> **Note that the dependence of the absolute effect on the comparison value also explains the "interaction with oneself" described in the chapter. The key insight is that the exp() in the link function turns the additive model $\small \alpha + \beta X$ into a multiplicative model.**


## Non-collapsibility of the odds ratios

Lets assume the following DPG X1 -> Y <- X2 where X1 and X2 have independent effects on Y:

```{r class.source = 'fold-show'}
n = 250
X1 = rnorm(n)
X2 = rnorm(n)
Y = as.integer(
  inv.logit(X1 + X2) > runif(n))
```

Now we can estimate the effect of `X1` as follows:

```{r class.source = 'fold-show'}
data.list = list(
  Y = Y,
  X1 = X1,
  X2 = X2
) 
model1 = alist(
  Y ~ dbinom(1,p),
  logit(p) <- a + X1*b1,
  a ~ dnorm(0,3),
  b1 ~ dnorm(0,3)
)
fit1 = quap(
  model1,
  data = data.list) 
precis(fit1)
```

Now lets include X2, which would be fine to do in a linear model.

```{r class.source = 'fold-show'}
data.list = list(
  Y = Y,
  X1 = X1,
  X2 = X2
) 
model2 = alist(
  Y ~ dbinom(1,p),
  logit(p) <- a + X1*b1 + X2*b2,
  a ~ dnorm(0,3),
  b1 ~ dnorm(0,3),
  b2 ~ dnorm(0,3)
)
fit2 = quap(
  model2,
  data = data.list) 
```
And we compare the posterior distribution for b2:

```{r class.source = 'fold-show'}
coeftab(fit1,fit2)
```

```{r, fig.height=3.5, fig.width=4, out.width="50%"}
pb1.1 = extract.samples(fit1)$b1
pb1.2 = extract.samples(fit2)$b1
set_par()
breaks = seq(
  min(c(pb1.1,pb1.2))-.0001,
  max(c(pb1.1,pb1.2))+.0001,
  length.out = 51)

hist(pb1.1, col = adjustcolor("blue",.5), border = NA, breaks = breaks, main = "")
hist(pb1.2, col = adjustcolor("red",.5), border = NA, breaks = breaks, add = TRUE)
```

**Differently than for linear regression, adding a non-confounder changes the effect on the odds-ratio scale.**
A related fact is that the average odds ratio of the sub-groups from a model with main and interaction effects of groups is not the same as the odds ratio over the totals sample. 

In all cases, the multiplicative effect of regression coefficients explains while **intuitions that are true for linear models (models with identity link function) are not valid when "exponential" linear link functions are involved.**

# Aggregated binomial regression

The binomial regression is the general case of which the logistic regression is a special case.
Both have a logistic link function, but whereas logistic regressions estimates if one trial was a "success", binomial regressions estimate how many out of N trials were successes.


To look at the aggregated binomial regression we will look at data from Norwegian national tests and ask the questions "Does the number of excused children depend on the topic English, reading or math?"

Here is the data:

```{r, echo = F, eval = F}
library(data.table)
NatTests5 =  
  fread("data/NasjonalTests5._trinn.csv") %>% 
  .[EnhetNivaa == 2,
    .(Fylke, EngN, EngExc, EngMiss,
      ReadN, ReadExc, ReadMiss,
      MathN, MathExc, MathMiss)] %>% 
  .[grepl("Troms",Fylke), Fylke := "Troms og Finnmark"] %>% 
  .[grepl("TrÃ¸ndelag",Fylke), Fylke := "TrÃ¸ndelag"] %>% 
  .[, EngN := as.numeric(gsub(" ","",EngN))] %>% 
  .[, ReadN := as.numeric(gsub(" ","",ReadN))] %>% 
  .[, MathN := as.numeric(gsub(" ","",MathN))] %>% 
  melt(id.vars = c("Fylke")) %>% 
  .[, topic := ifelse(grepl("Math",variable),"Math",ifelse(grepl("Eng",variable),"Eng","Read"))] %>% 
  .[, variable := gsub("Math|Eng|Read","",variable)] %>% 
  dcast(Fylke + topic ~ variable) %>% 
  .[, N_total := round(max(100/(100-(Exc+Miss))*N)), by = .(Fylke)] %>% 
  .[, `:=`(N_Exc  = round(N_total * Exc / 100))] %>% 
  .[, `:=`(N_Miss  = N_total-N_Exc-N)] %>% 
  setnames("N","N_part") %>% 
  .[, .(Fylke, topic, N_total, N_part, N_Miss, N_Exc)] %>% 
  .[, Fylke_Nr := rank(-N_total), by = .(topic)] %>% 
  .[, topic := factor(topic)] %>% 
  .[, topic_Nr := as.numeric(topic)] %>% 
  setkeyv(c("Fylke_Nr","topic_Nr")) %>% 
  as.data.frame()

save(NatTests5,file = "data/NasjonalTests5.Rdata")
```

```{r class.source = 'fold-show'}
load(file = "data/NasjonalTests5.Rdata")
NatTests5 
```


To set up a binomial model, we set up a model that 

- allows the number of trials (total number of students) vary from observation to observation. 
- allows different intercepts for different Fylke, using the indexing approach
- estimates a global effect of topic across all Fylke, using the indexing approach.

Here is the model: 

```{r class.source = 'fold-show'}
NT.model = 
  alist(
    N_Exc ~ dbinom(N_total, p),
    logit(p) <- a[Fylke_Nr] + b[topic_Nr],
    a[Fylke_Nr] ~ dnorm(-1,1),
    b[topic_Nr] ~ dnorm(0,1))
```

Here we are fitting the model.

```{r class.source = 'fold-show', results='hide', message=FALSE, warning=FALSE}
u.fit_NT = ulam(
  NT.model,
  data = NatTests5,
  log_lik = TRUE,   # for model comparison
  iter = 2000,      # 2000 iterations, (1000 warmup)
  chains = 4,       # four chains, to check convergence
  cores = 4,        # use 4 cores in parallel
  cmdstan = TRUE)   # use cmdstanr not rstan
```

```{r}
precis(u.fit_NT, depth = 2) %>% round(2)
```

n_eff and Rhat don't look great. We look at trace-plots for the first a and b to figure out what is going on:

```{r class.source = 'fold-show', message=FALSE, warning=FALSE}
library(posterior)
library(bayesplot)
draws = u.fit_NT@stanfit %>% as_draws()
draws %>% subset_draws(c("a[1]","b[1]")) %>% mcmc_trace()
```

The chains are not mixed as well as one would hope, and it looks a bit like the chains are correlated. We examine this with a pairs plot:

```{r class.source = 'fold-show'}
draws %>% subset_draws(c("a[1]","b[1]")) %>% mcmc_pairs()
```

The strong negative correlation reminds us that the Fylke and topic intercepts both already represent proportion excused children on their own. With these priors, the sum of the Fylke and topic effect will estimate the right proportion of exclusions, but we will be very uncertain about the Fylke and topic effects because many combinations of Fylke and topic intercept can result in the same total proportion excluded students.

While we can estimate such a model in a Bayesian way, the standard deviation of the correlated parameters will be inflated. (Parameter comparisons will not be affected.) 

We can solve this by re-formulating the model and use the indexing approach only for two of the three of the topics.

We update our data so that there is one dummy variable for Math and one for English tests.

```{r class.source = 'fold-show'}
NatTests5$Math = 1*(NatTests5$topic == "Math")
NatTests5$Eng = 1*(NatTests5$topic == "Eng")
```

And we formulate and estimate an updated model, where the Fylke-intercepts will capture excuses for reading.

```{r class.source = 'fold-show', results='hide', message=FALSE, warning=FALSE}
NT.model2 = 
  alist(
    N_Exc ~ dbinom(N_total, p),
    logit(p) <- a[Fylke_Nr] + b1*Math + b2*Eng,
    a[Fylke_Nr] ~ dnorm(-1,1),
    b1 ~ dnorm(0,1),
    b2 ~ dnorm(0,1))
fn = "ufit_NT2.Rdata" 
if (file.exists(fn)) {
  load(fn)
} else {
  u.fit_NT2 = ulam(
  NT.model2,
  data = NatTests5,
  log_lik = TRUE,   # for model comparison
  iter = 2000,      # 2000 iterations, (1000 warmup)
  chains = 4,       # four chains, to check convergence
  cores = 4,        # use 4 cores in parallel
  cmdstan = TRUE)   # use cmdstanr not rstan
  save(u.fit_NT2, file = fn)
}

```

```{r}
precis(u.fit_NT2, depth = 2) %>% round(2)
```


This looks much better. 

Here is a posterior predictive plot with observed data in black, red and green^[should not be a problem for color blind viewers, see [here](https://developer.r-project.org/Blog/public/2019/11/21/a-new-palette-for-r/)], and the predicted values in white with blue border.

```{r, fig.height=7}
NatTests5$obs = NatTests5$N_Exc/NatTests5$N_total
NatTests5$x = NatTests5$Fylke_Nr + (NatTests5$topic_Nr-2)/8

set_par(mar=c(3,1,.5,.5))

with(NatTests5, 
     plot(obs, rev(x), cex = 2, col = topic_Nr,
          pch = 16, xlim = c(0,.1),
          yaxt = "n", ylab = "", xlab = "proportion excused")
     )

with(NatTests5[NatTests5$topic == "Math",],
     text(0, rev(x), Fylke, pos = 4)
     )

legend("topright", col = 1:3, 
       legend = c("English","Math","Reading"), bty = "n", pch = 16)

pp = link(u.fit_NT2)
CI = apply(pp,2,PI)
arrows(y0 = rev(NatTests5$x), x0 = CI[1,], x1 = CI[2,], col = "blue", length = 0)
points(colMeans(pp), rev(NatTests5$x), cex = 1.25,
       col = "blue", pch = 21, bg = "white")
```


What do the coefficients tell us?

```{r}
Fx = unique(NatTests5$Fylke)
coefs = coeftab(u.fit_NT2)@coefs

```


- The intercepts vary between `r min(coefs[1:11])` for `r Fx[which.min(coefs[1:11])]` , i.e. the probability to be excused from the reading test is `exp(min(coefs[1:11]))` = `r round(exp(min(coefs[1:11])),2)`, 
- and `r max(coefs[1:11])` for for `r Fx[which.max(coefs[1:11])]`, i.e. the probability to be excused from the reading test is `exp(max(coefs[1:11]))` = `r round(exp(max(coefs[1:11])),2)`. 
- This means that the chance to be excused from the reading test is `exp(max(coefs[1:11]))/exp(min(coefs[1:11])) = 
`r round(exp(max(coefs[1:11]))/exp(min(coefs[1:11])),2)` larger in MÃ¸re og Romsdal, compared to Viken.

Here is a plot with the log odds to be excused from the reading test by Fylke. This are just the intercepts!

```{r}
plot(precis(u.fit_NT2, depth = 2, pars = "a"))
yy = unique(NatTests5[,c("Fylke","Fylke_Nr")])
text(-2.4,rev(yy$Fylke_Nr), yy$Fylke,pos = 3)
```

Regarding the effect of topic we find that

```{r, fig.height=2, out.height="30%"}
plot(precis(u.fit_NT2, pars = c("b1","b2")))
```

- the log odds ratio for math is -.17, i.e. the odds ratio is `exp(-.17)` = `r round(exp(-.17),2)`. Because the baseline probability for reading is low, we can simply say that the relative risk to be excused from the math test is, *compared to reading*, `r round(exp(-.17),2)`
- the log odds ratio for English is -.08, i.e. the odds ratio is `exp(-.08)` = `r round(exp(-.08),2)`. Because the baseline probability for reading is low, we can simply say that the relative risk to be excused from the math test is, *compared to reading*, `r round(exp(-.08),2)`

# Summary

- The GLM extends the linear (Gaussian) model by allowing for different outcome distributions. Outcome distributions that are constrained to be in a certain range require link functions.
- Priors for models with link functions can have un-intuitive effects, which makes prior posterior checks especially important.
- The exponent in the inverse logit (logistic) link function makes that the model parameters have a multiplicative effects on the outcome scale. Therefor, care is required when interpreting results from such models.
- Odds ratios obtained from logistic regressions are good approximations to risk ratios or relative risks when the baseline probability is low (< ~ 10%).
- Simpsons paradox exists in psychology! [Here](https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00513/full) is a paper only about that by Rogier Kievit et al..

# Likelihood and model comparison

- Do not compare models that use different likelihood functions

When using the aggregate or disaggregate models we do not leave trials out, but observations. For instances, assume you ran an experiment with 4 conditions and 10 observations in each condition. If you analyse the data with only the experimental condition as a predictor, you can structure the data in 2 ways

- Disaggregate: 40 rows, each row indicating if the one "trial" was successful
- Aggregate: 4 rows, each row indicating how many out of 10 trials were successful

For LOO, one would then either leave one individual out at the time for the disaggregate data, but one experimental group for the aggregate data. However, this would not influence the relative LOO-CV values of different models, as in the disaggregate data we would leave out individuals from each experimental group equally often.

In this example, it makes only sense to use the aggregate version, if we assume that the individuals in the four conditions are identical regarding characteristics relevant for the experiment. However, if e.g. we had in each group half men and half women, we could aggregate into 8 groups!